{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XaV7YTqhRuhR",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:04:59.827597300Z",
     "start_time": "2023-08-04T12:04:59.765753400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DRVzyjl5Ry4V",
    "outputId": "68b79353-88be-44d0-fb78-16537668ab98",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:00.963530300Z",
     "start_time": "2023-08-04T12:04:59.776594900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0VCrGCDOlrm"
   },
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LV7Y_5c_Rw8m",
    "outputId": "0b7353c6-a196-444a-9a9d-08d637354dd1",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:00.966358900Z",
     "start_time": "2023-08-04T12:05:00.964038100Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"data/\")\n",
    "image_path = Path(data_path / \"Rock-Paper-Scissors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.010941200Z",
     "start_time": "2023-08-04T12:05:00.968759400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if \"Rock-Paper-Scissors\" not in os.listdir(data_path):\n",
    "    with ZipFile(data_path / \"archive.zip\", \"r\") as zipfile:\n",
    "        zipfile.extractall(data_path)\n",
    "else:\n",
    "    print(os.listdir(data_path))\n",
    "    print(\"File is already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5XiA4gZbR1J1",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.016082200Z",
     "start_time": "2023-08-04T12:05:05.012456300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpHePJIcOsrx"
   },
   "source": [
    "## Adding Custom Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HO3IICoJW3u4",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.024177800Z",
     "start_time": "2023-08-04T12:05:05.018505800Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((224, 224)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    transforms.ColorJitter(contrast=2.0),\n",
    "    transforms.GaussianBlur(5, sigma=(0.1, 2.0)),\n",
    "    \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.Grayscale(num_output_channels=3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9RUyhzVlTHAZ",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.136509400Z",
     "start_time": "2023-08-04T12:05:05.024177800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KGrqmbxQWwiN",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.142951700Z",
     "start_time": "2023-08-04T12:05:05.136509400Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLVF3LigOxGh"
   },
   "source": [
    "## Loading ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpUHiReHXJCy",
    "outputId": "d576dad3-e75e-42ab-b395-9c6bfaed14d3",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.441909100Z",
     "start_time": "2023-08-04T12:05:05.144349100Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jIXInZEMXbMK",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:05.445443700Z",
     "start_time": "2023-08-04T12:05:05.444014Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "in_features = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fX-8xdqqXe1B",
    "outputId": "8be07041-4cc5-48ec-d65a-1af9fa9e87ab",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:06.198370400Z",
     "start_time": "2023-08-04T12:05:05.445443700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z7N1M8TO3U9"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rfXClwR5Xjl2",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:05:06.200993900Z",
     "start_time": "2023-08-04T12:05:06.198878Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBZx7375Xqwz",
    "outputId": "94b4909b-139c-4f39-ab37-8d3a100292da",
    "ExecuteTime": {
     "end_time": "2023-08-04T12:08:04.282773700Z",
     "start_time": "2023-08-04T12:05:06.200993900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "703b456ba90d4000ae8a4c495887dbb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f9e11e8933046f2b0a15d04403ea200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "980052f448814dfc86f3f54eb6e628e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dddd286c767e4137a76d181430f3d141"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c53a0a3fdc14e73b72a64162493823f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2be9fb41b406409eb0633246231a6f26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f15725defb346b9b29b5020daeac7b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c8f6f1a6cc406ab5412492588eb099"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e858c5816efa4138833e0c2439daa7b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {epoch}\".format(epoch=epoch+1))\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet18(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"resnet-trained-model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torch.load(PATH)\n",
    "resnet18.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsRLl2Q-O9-p"
   },
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3Zmd7fVkXywn"
   },
   "outputs": [],
   "source": [
    "resnet18.eval()\n",
    "\n",
    "embeddings, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, batch_labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        outputs = resnet18(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        embeddings.extend(outputs.cpu().numpy())\n",
    "        labels.extend(batch_labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ArMwnYJY83V",
    "outputId": "5a9154de-d8e7-405d-90a3-a5fb82ab348e"
   },
   "outputs": [],
   "source": [
    "total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model achieved an accuracy of: {:.2f}%\".format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting t-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSGRQg36c7t8",
    "outputId": "c0a67b85-9ea5-410e-a460-7372e2e2ad48"
   },
   "outputs": [],
   "source": [
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C8a6z78eIvf",
    "outputId": "69e884fc-36d0-4084-a463-fbb8cde8e50d"
   },
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaYiwSPyMjIT",
    "outputId": "a4360d85-c0bd-4bb7-f5bf-98955b985302"
   },
   "outputs": [],
   "source": [
    "n_classes = test_dataset.classes\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colormaps\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_result = TSNE(n_components=3, learning_rate=\"auto\", init='random', n_iter=10000, perplexity=30).fit_transform(np.array(embeddings))\n",
    "\n",
    "cmap = colormaps.get_cmap('viridis')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap=cmap, s=20)\n",
    "\n",
    "plt.title('TSNE Visualization of ResNet18 Features')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks(np.arange(0, 3))\n",
    "cbar.set_ticklabels(n_classes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5r228VQPH1w"
   },
   "source": [
    "## Defining Grad-Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "l4oSO-SGlvuU",
    "outputId": "f3e62129-3a16-4f45-e0d7-6a89d5e67762"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_grad_cam(image, model, target_layer):\n",
    "    model.eval()\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        feature_map = input[0]\n",
    "        grad_value = output[0]\n",
    "\n",
    "        grad_cam_features.append(feature_map)\n",
    "        grad_cam_grads.append(grad_value)\n",
    "\n",
    "    grad_cam_features = []\n",
    "    grad_cam_grads = []\n",
    "\n",
    "    hook_handle = target_layer.register_forward_hook(forward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    image = image.to(device)\n",
    "    output = model(image.unsqueeze(0))\n",
    "    predicted_class = torch.argmax(output)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    output[0, predicted_class].backward()\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    alpha_kc = torch.mean(grad_cam_grads[0].squeeze(), dim=[1, 2], keepdim=True)\n",
    "    grad_cam = torch.mean((alpha_kc * grad_cam_features[0]), dim=1).squeeze()\n",
    "    grad_cam = nn.ReLU()(grad_cam)\n",
    "\n",
    "    grad_cam = grad_cam.detach().cpu().numpy()\n",
    "    grad_cam = (grad_cam - np.min(grad_cam)) / (np.max(grad_cam) - np.min(grad_cam))\n",
    "\n",
    "    return grad_cam\n",
    "\n",
    "\n",
    "def overlay_grad_cam(image, grad_cam):\n",
    "    heatmap = cv2.resize(grad_cam, (image.shape[2], image.shape[1]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 200\n",
    "\n",
    "    overlaid_img = heatmap + np.float32(image.permute(1, 2, 0))\n",
    "    overlaid_img = overlaid_img / np.max(overlaid_img)\n",
    "\n",
    "    return overlaid_img\n",
    "\n",
    "\n",
    "def plot_grad_cam_heatmap(grad_cam, overlaid_img):\n",
    "    plt.imshow(overlaid_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "target_layer = resnet18.layer1[-1].conv2\n",
    "\n",
    "sample_image, _ = test_dataset[1]\n",
    "sample_image = sample_image.to(device)\n",
    "outputs = resnet18(sample_image.unsqueeze(0))\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(outputs.data)\n",
    "print(n_classes[predicted])\n",
    "\n",
    "grad_cam = get_grad_cam(sample_image, resnet18, target_layer)\n",
    "overlaid_img = overlay_grad_cam(sample_image.cpu(), grad_cam)\n",
    "plot_grad_cam_heatmap(grad_cam, overlaid_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Grad-Cam Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "   \n",
    "cap= cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    ret,frame = cap.read()        \n",
    "    \n",
    "    sample_image = transforms.ToTensor()(frame).to(device)\n",
    "\n",
    "    outputs = resnet18(sample_image.unsqueeze(0))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    grad_cam = get_grad_cam(sample_image, resnet18, target_layer)\n",
    "    overlaid_img = overlay_grad_cam(sample_image.cpu(), grad_cam)\n",
    "    \n",
    "    print(outputs.data)\n",
    "    print(n_classes[predicted])\n",
    "\n",
    "    cv2.putText(overlaid_img, n_classes[predicted], (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"image\", overlaid_img)\n",
    "\n",
    "    if cv2.waitKey(2) & 0xff == ord('q'):\n",
    "         break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
